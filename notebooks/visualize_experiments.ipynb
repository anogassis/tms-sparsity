{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import ConvexHull\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "from tms.utils.utils import load_results\n",
    "from tms.training.experiments import run_experiments\n",
    "from tms.utils.utils import generate_sparsity_values\n",
    "from tms.data.dataset import SyntheticBinaryValued\n",
    "from tms.models.autoencoder import ToyAutoencoder\n",
    "from tms.llc import estimate_llc, get_llc_data\n",
    "from tms.plots.kgons import plot_losses_and_polygons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"debug_1.13.0\"\n",
    "data_path = \"../data\"\n",
    "results_1_13 = load_results(data_path, version)\n",
    "\n",
    "# llc_estimates = estimate_llc(results, version)\n",
    "llc_estimates_1_13 = get_llc_data(results_1_13, version, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"debug_1.14.0\"\n",
    "data_path = \"../data\"\n",
    "results_1_14 = load_results(data_path, version)\n",
    "\n",
    "# llc_estimates = estimate_llc(results, version)\n",
    "llc_estimates_1_14 = get_llc_data(results_1_14, version, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataframes_and_results(df_results_pairs, positions = [9, 18, 27, 36, 45], hyperparam_combos = [(300, 0.001)], x_scale=\"linear\", y_scale=\"linear\", sharey=False, sharex=False, ymin=1e-4):\n",
    "    warnings.simplefilter(action='ignore', category=UserWarning)  # Probably unwise\n",
    "\n",
    "    for batch_size, learning_rate in hyperparam_combos:\n",
    "        print(f\"Batch size: {batch_size}, Learning rate: {learning_rate}\\n\")\n",
    "        \n",
    "        for position in positions:\n",
    "            fig, axes = plt.subplots(1, len(df_results_pairs), figsize=(15*len(df_results_pairs), 10), sharey=sharey, sharex=sharex)\n",
    "            if len(df_results_pairs) == 1:\n",
    "                axes = [axes]\n",
    "\n",
    "            for pair_index, (llc_estimates, results) in enumerate(df_results_pairs):\n",
    "                llc_loss_by_sparsity = defaultdict(list)\n",
    "                steps = results[0]['parameters']['log_ivl']\n",
    "                \n",
    "                for index in range(len(results)):\n",
    "                    filtered_df = llc_estimates[llc_estimates['index'] == index]\n",
    "                    llc = filtered_df[\n",
    "                        (filtered_df['batch_size'] == batch_size) &\n",
    "                        (filtered_df['lr'] == learning_rate) &\n",
    "                        (filtered_df['snapshot_index'] == position) &\n",
    "                        (filtered_df['t_sgld'] > 150) &\n",
    "                        (filtered_df['llc_type'] != \"mean\")\n",
    "                    ][\"llc\"].mean()\n",
    "                    \n",
    "                    loss = results[index]['logs']['loss'].values[position]\n",
    "                    sparsity = results[index]['parameters']['sparsity']\n",
    "                    llc_loss_by_sparsity[sparsity].append((llc, loss))\n",
    "\n",
    "                for sparsity, llc_loss in llc_loss_by_sparsity.items():\n",
    "                    if sparsity == 0:\n",
    "                        continue\n",
    "                    llcs = [llc for llc, loss in llc_loss if not(np.isnan(llc))]\n",
    "                    losses = [loss for llc, loss in llc_loss if not(np.isnan(llc))]\n",
    "                    if all(np.isnan(llc) for llc in llcs):\n",
    "                        continue\n",
    "                    axes[pair_index].scatter(*zip(*llc_loss), label=f\"Sparsity: {round(sparsity,3)}\")\n",
    "                    if pair_index == 0:\n",
    "                        title = \"Initialized at random 4-gon\"\n",
    "                    if pair_index == 1:\n",
    "                        title = \"Initialized at optimal parameters for sparse inputs\"\n",
    "                    axes[pair_index].set_title(f\"Pair {title}, Position {position}\")\n",
    "                    axes[pair_index].set_xlabel(\"LLC\")\n",
    "                    axes[pair_index].set_ylabel(\"Loss\")\n",
    "                    axes[pair_index].legend()\n",
    "                    axes[pair_index].set_xscale(x_scale)\n",
    "                    axes[pair_index].set_yscale(y_scale)\n",
    "                    axes[pair_index].set_ylim(ymin=ymin)\n",
    "\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f\"Loss and LLC After Epoch {steps[position]}\", fontsize=16)\n",
    "            plt.subplots_adjust(top=0.9)\n",
    "            #plt.savefig(f'../results/loss_vs_llc_epoch_{steps[position]}')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_sparsity_1_13 = llc_estimates_1_13[llc_estimates_1_13['index'] >= 400]\n",
    "high_sparsity_1_14 = llc_estimates_1_14[llc_estimates_1_14['index'] >= 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_dataframes_and_results([(llc_estimates_1_13, results_1_13), (llc_estimates_1_14, results_1_14)], hyperparam_combos=[(300, 0.001)], y_scale=\"log\", ymin=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_dataframes_and_results([(llc_estimates_1_13, results_1_13), (llc_estimates_1_14, results_1_14)], positions=[36,45], hyperparam_combos=[(300, 0.001)], y_scale=\"linear\", sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, plot_number =5, step =-1, loss_window = (0.14, .16), weird_indices = [], sparsities= [0.426, 0.671, 0.811, 0.892, 0.938, 0.964, 0.98 , 0.988, 0.993], epsilon=0.001):\n",
    "    # loss_hist = []\n",
    "    for sparse_value in sparsities:\n",
    "        plotted =0\n",
    "        print(f\"Plot polygons for sparsity={sparse_value}\")\n",
    "        for index in range(len(results)):\n",
    "            \n",
    "            STEPS = results[index]['parameters']['log_ivl']\n",
    "            logs = results[index]['logs']\n",
    "            losses = [logs.loc[logs['step'] == s, 'loss'].values[0] for s in STEPS]\n",
    "\n",
    "            outside_loss_window = losses[step] < loss_window[0] or losses[step] > loss_window[1]\n",
    "            non_matching_sparsity = abs(results[index]['parameters']['sparsity'] - sparse_value) > epsilon\n",
    "            if non_matching_sparsity or outside_loss_window:\n",
    "                continue\n",
    "            else:\n",
    "                if plotted>=plot_number:\n",
    "                    continue\n",
    "                plotted+=1\n",
    "\n",
    "\n",
    "            NUM_EPOCHS = 20000\n",
    "            PLOT_STEPS = [min(STEPS, key=lambda s: abs(s-i)) for i in [0, 200, 2000, 10000, NUM_EPOCHS - 1]]\n",
    "            PLOT_INDICES = [STEPS.index(s) for s in PLOT_STEPS]\n",
    "            Ws = [results[index]['weights'][i]['embedding.weight'] for i in PLOT_INDICES]\n",
    "            biases = [results[index]['weights'][i]['unembedding.bias'] for i in PLOT_INDICES]\n",
    "            \n",
    "            model = ToyAutoencoder(6, 2, final_bias=True)\n",
    "            new_weights ={}\n",
    "            for idx, ndarray in results[index]['weights'][PLOT_INDICES[-1]].items():\n",
    "                new_weights[idx] = torch.from_numpy(ndarray)\n",
    "\n",
    "            # criterion=nn.MSELoss()\n",
    "        \n",
    "            # model.load_state_dict(new_weights)\n",
    "\n",
    "            # # print(sample)\n",
    "            # # print(model(sample))\n",
    "            # test_set = SyntheticBinaryValued(10000, 6, sparse_value)\n",
    "            # mean_loss_test = 0\n",
    "            # for sample in test_set:\n",
    "            #     output = model(sample)\n",
    "            #     mean_loss_test += criterion(output, sample)\n",
    "            # # print(\"Mean loss test:\")\n",
    "            # loss_hist.append(mean_loss_test)\n",
    "            # print(f\"index: {index}\")\n",
    "            # print(mean_loss_test/10000)\n",
    "            # # if mean_loss_test<20:\n",
    "            # #     continue\n",
    "            # # else:\n",
    "            # #     weird_indices.append(index)\n",
    "            \n",
    "\n",
    "            #all_weights = [[results[j]['weights'][i] for i in PLOT_INDICES] for j in range(len(results))]\n",
    "            plot_losses_and_polygons(STEPS, losses, PLOT_STEPS, Ws, biases)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_1_13, plot_number=2, loss_window=(0, 1), sparsities=[0.993, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_1_14, plot_number=2, loss_window=(0, 1), sparsities=[0.993, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_1_14[1]['dataset_test'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_convex_hull_vertices(W):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the number of vertices of the convex hull of the points represented by the columns of W.\n",
    "    \n",
    "    Parameters:\n",
    "    W (torch.Tensor): A 2xN matrix where each column represents a point in 2D space.\n",
    "    \n",
    "    Returns:\n",
    "    int: The number of vertices of the convex hull.\n",
    "    \"\"\"\n",
    "    if W.shape[0] != 2:\n",
    "        raise ValueError(\"The weight matrix W must have 2 rows.\")\n",
    "    \n",
    "    # Convert the tensor to a numpy array if it isn't already\n",
    "    if isinstance(W, torch.Tensor):\n",
    "        W = W.cpu().detach().numpy()\n",
    "    \n",
    "    hull = ConvexHull(W.T)\n",
    "    return len(hull.vertices)  # The number of vertices is the same as the number of edges\n",
    "\n",
    "def count_kgons(W):\n",
    "    edge_counts = {}\n",
    "    \n",
    "    # Process each weight matrix\n",
    "    for full_w in W:\n",
    "        num_edges = classify_kgon(full_w)\n",
    "        if num_edges in edge_counts:\n",
    "            edge_counts[num_edges] += 1\n",
    "        else:\n",
    "            edge_counts[num_edges] = 1\n",
    "\n",
    "    return edge_counts\n",
    "\n",
    "def classify_5_gon(W, b, differentiate_5_plus=False):\n",
    "    \"\"\"\n",
    "    Classify a 5-gon based on the weights and biases. \n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tensor to numpy if it isn't already\n",
    "    if isinstance(W, torch.Tensor):\n",
    "        W = W.cpu().detach().numpy()\n",
    "    \n",
    "    if W.shape[0] == 2:\n",
    "        W = W.T\n",
    "\n",
    "    # Compute the convex hull\n",
    "    hull = ConvexHull(W)\n",
    "    \n",
    "    # Check if the number of vertices is equal to 5\n",
    "    if len(hull.vertices) != 5:\n",
    "        return \"not a 5-gon\"\n",
    "    \n",
    "    # Convert biases to a numpy array if it isn't already\n",
    "    if isinstance(b, torch.Tensor):\n",
    "        b = b.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    # Check if any of the non-vertex biases are large negative\n",
    "    non_vertex_biases = np.delete(b, hull.vertices)\n",
    "\n",
    "    # Check for any positive bias that is not part of the convex hull vertices\n",
    "    non_hull_positive_bias = np.any(non_vertex_biases > 0)\n",
    "\n",
    "    if not non_hull_positive_bias:\n",
    "        return 5\n",
    "    elif non_hull_positive_bias and differentiate_5_plus:\n",
    "        return \"5+\"\n",
    "    elif non_hull_positive_bias and not differentiate_5_plus:\n",
    "        return 5\n",
    "    else:\n",
    "\n",
    "        return 'not a 5-gon'\n",
    "\n",
    "\n",
    "\n",
    "def classify_kgon(W):\n",
    "    embedding_w = W[\"embedding.weight\"]\n",
    "    edges = calculate_convex_hull_vertices(embedding_w)\n",
    "    if edges == 5:\n",
    "        return classify_5_gon(embedding_w, W[\"unembedding.bias\"])\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kgon_percentages(results, step =-1, sparsities= [0.0, 0.426, 0.671, 0.811, 0.892, 0.938, 0.964, 0.98 , 0.988, 0.993], epsilon=0.001):\n",
    "    # loss_hist = []\n",
    "    for sparse_value in sparsities:\n",
    "        plotted =0\n",
    "        print(f\"Plot polygons for sparsity={sparse_value}\")\n",
    "        weights = []\n",
    "\n",
    "        STEPS = results[0]['parameters']['log_ivl']\n",
    "        NUM_EPOCHS = 20000\n",
    "        PLOT_STEPS = [min(STEPS, key=lambda s: abs(s-i)) for i in [0, 200, 2000, 10000, NUM_EPOCHS - 1]]\n",
    "        PLOT_INDICES = [STEPS.index(s) for s in PLOT_STEPS]\n",
    "        for PLOT_INDEX in PLOT_INDICES:\n",
    "            weights = []\n",
    "            for index in range(len(results)):\n",
    "                non_matching_sparsity = abs(results[index]['parameters']['sparsity'] - sparse_value) > epsilon\n",
    "                if non_matching_sparsity:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                # Ws = [results[index]['weights'][i]['embedding.weight'] for i in PLOT_INDICES]\n",
    "                # weights = [results[index]['weights'][i] for i in PLOT_INDICES]\n",
    "\n",
    "                \n",
    "                weights.append(results[index]['weights'][PLOT_INDEX])\n",
    "            print(count_kgons(weights))\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_kgon_percentages(\n",
    "    results_1_13,\n",
    "    #k_values = [3, 4, 5, 6],\n",
    "    # sparsities= [0.426],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_kgon_percentages(\n",
    "    results_1_14,\n",
    "    #k_values = [3, 4, 5, 6],\n",
    "    # sparsities= [0.426],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataframes_and_results2(df_results_pairs, positions = [9, 18, 27, 36, -1], hyperparam_combos = [(300, 0.001)], x_scale=\"linear\", y_scale=\"linear\", sharey=False, sharex=False, ymin=1e-4, xmin=-50):\n",
    "    warnings.simplefilter(action='ignore', category=UserWarning)  # Probably unwise\n",
    "\n",
    "    for batch_size, learning_rate in hyperparam_combos:\n",
    "        print(f\"Batch size: {batch_size}, Learning rate: {learning_rate}\\n\")\n",
    "        \n",
    "        fig, axes = plt.subplots(len(positions), len(df_results_pairs), figsize=(15*len(df_results_pairs), 6* len(positions)), sharey=sharey, sharex=sharex)\n",
    "        for i, position in enumerate(positions):\n",
    "            # fig, axes = plt.subplots(1, len(df_results_pairs), figsize=(15*len(df_results_pairs), 10), sharey=sharey, sharex=sharex)\n",
    "            if len(df_results_pairs) == 1:\n",
    "                axes = [axes]\n",
    "\n",
    "            for pair_index, (llc_estimates, results) in enumerate(df_results_pairs):\n",
    "                llc_loss_by_sparsity = defaultdict(list)\n",
    "                steps = results[0]['parameters']['log_ivl']\n",
    "                \n",
    "                for index in range(len(results)):\n",
    "                    filtered_df = llc_estimates[llc_estimates['index'] == index]\n",
    "                    llc = filtered_df[\n",
    "                        (filtered_df['batch_size'] == batch_size) &\n",
    "                        (filtered_df['lr'] == learning_rate) &\n",
    "                        (filtered_df['snapshot_index'] == position) &\n",
    "                        (filtered_df['t_sgld'] > 150) &\n",
    "                        (filtered_df['llc_type'] != \"mean\")\n",
    "                    ][\"llc\"].mean()\n",
    "                    \n",
    "                    loss = results[index]['logs']['loss'].values[position]\n",
    "                    sparsity = results[index]['parameters']['sparsity']\n",
    "                    llc_loss_by_sparsity[sparsity].append((llc, loss))\n",
    "\n",
    "                for sparsity, llc_loss in llc_loss_by_sparsity.items():\n",
    "                    if sparsity == 0:\n",
    "                        continue\n",
    "                    llcs = [llc for llc, loss in llc_loss if not(np.isnan(llc))]\n",
    "                    losses = [loss for llc, loss in llc_loss if not(np.isnan(llc))]\n",
    "                    if all(np.isnan(llc) for llc in llcs):\n",
    "                        continue\n",
    "                    axes[i,pair_index].scatter(*zip(*llc_loss), label=f\"Sparsity: {round(sparsity,3)}\")\n",
    "                    if pair_index == 0:\n",
    "                        title = \"Initialized at random 4-gon\"\n",
    "                    if pair_index == 1:\n",
    "                        title = \"Initialized at optimal parameters for sparse inputs\"\n",
    "                    axes[i,pair_index].set_title(f\"Pair {title}, Position {position}\")\n",
    "                    axes[i,pair_index].set_xlabel(\"LLC\")\n",
    "                    axes[i,pair_index].set_ylabel(\"Loss\")\n",
    "                    axes[i,pair_index].legend()\n",
    "                    axes[i,pair_index].set_xscale(x_scale)\n",
    "                    axes[i,pair_index].set_yscale(y_scale)\n",
    "                    axes[i,pair_index].set_ylim(ymin=ymin)\n",
    "                    axes[i,pair_index].set_xlim(xmin=xmin)\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(f\"Loss and LLC After Epoch {steps[position]}\", fontsize=16)\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        #plt.savefig(f'../results/loss_vs_llc_epoch_{steps[position]}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_dataframes_and_results2([(high_sparsity, results), (high_sparsity2, results2)], positions=[9, 18, 27, 36], hyperparam_combos=[(300, 0.001)], y_scale=\"log\", sharex=True, sharey=True, ymin=1e-11, xmin=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
